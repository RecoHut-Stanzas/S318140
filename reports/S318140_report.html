<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Providing Bundled Product Recommendations to Online E-Commerce Customers</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
}

.simple-table-header {
	background: rgb(247, 246, 243);
	color: black;
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(145, 145, 142, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(187, 132, 108, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(215, 129, 58, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 148, 51, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(108, 155, 125, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(91, 151, 189, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(167, 130, 195, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(205, 116, 159, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(225, 111, 100, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(145, 145, 142, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(187, 132, 108, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(215, 129, 58, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 148, 51, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(108, 155, 125, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(91, 151, 189, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(167, 130, 195, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(205, 116, 159, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(225, 111, 100, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="98c81e48-d476-4a6b-b774-b523183f3ff2" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">💊</span></div><h1 class="page-title">Providing Bundled Product Recommendations to Online E-Commerce Customers</h1></header><div class="page-body"><p id="736ed9e3-35cb-4053-8b04-a99ef7840e3e" class=""><em><strong><mark class="highlight-gray">Predicting each user’s purchasing feedback to nine exposed items.</mark></strong></em></p><h2 id="bff75c2c-de95-4e0f-a635-6848214db23c" class="">TL;DR</h2><ol type="1" id="4a8fcee4-8d3d-4737-8de1-6dda765f9113" class="numbered-list" start="1"><li>This report is based on <em>IEEE BigData Cup 2021 Challenge Track 1.</em></li></ol><hr id="7a5ec845-860f-41e5-92ff-25081330143c"/><nav id="a2d17e8e-bc95-432c-b4ee-173bb30f6fc3" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#bff75c2c-de95-4e0f-a635-6848214db23c">TL;DR</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#65b230ee-d706-4522-bca7-b25f9f706952">Data Exploration</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#dbf4e5cf-7bbb-4709-bbad-2542ddb80bb0">Overall data statistics</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#961f0891-87ac-40ec-86e2-278f4d44490e">Click and buy statistics in different sessions</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#a9ff1b9c-11de-49c0-a0a7-359db16daa5b">Buying behavior analysis</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#fbf1a9bb-70f1-4f61-b868-53699c4d128b">Click behavior analysis</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#890e388e-9389-42cf-8060-d79d2ba18a79">User portrait features</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#23273250-6536-46e4-88dd-59726405a5bc">Item features</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#e96d495e-92e9-4bcc-9b81-3dbee4c01ecb">Data snapshots</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#7100d80a-545b-4693-b331-867f276279f8">Data Preprocessing</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#bc6c8cc9-a59c-4781-b7b3-5df616402e68">Item Info Processing</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#649e4771-7318-43b7-a71f-0dced86453ec">Train data processing</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#f814079b-d115-437e-b58c-59a1288ec477">Test data processing</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1074d92c-5890-4cf6-a682-2f597a3ccbce">Item discretization</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#60f5abf1-0aaf-45b7-8624-03bbd4f6db76">Training dataset preparation (torch Tensor Dataset format)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#bacac4b4-edc3-49c9-b6d4-69da1cdca1c3">Testing dataset preparation (torch Tensor Dataset format)</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#ee8713dc-8cf7-4907-b6c1-d5f947c8fe07">MLP Model</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#39d0c9d8-34a5-43bf-b007-5f407d4c4696">Architecture</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#67691604-5511-4827-bdae-010f641d6ae5">Implementation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#5e350009-2f1b-4182-89f4-094bd1ce5b12">Training and Validation Output</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#4830e045-b2c5-472a-998f-653fce275b18">Session Prediction</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#79ffad49-ea1a-47fc-bc0e-78bc9cddaa0d">Architecture</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#4406796d-b2cb-40bb-8edb-0835a845affb">Implementation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#2f18eb32-9502-495e-b551-25e8fa138015">Training and Validation Output</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#302441c8-fcc7-4998-b16a-d658d30b85a6">Prediction Accuracy</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#24c875ee-48bd-4c81-942f-a5c5435b2c22">Feature Layer Augmentation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d1ff7273-7b0a-400f-982c-24f159494c04">Implementation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9cc3db37-e8ac-4827-b604-a6d6bb2d252d">Performance</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1a6db28a-1c80-4dbd-95dd-d23b1a04affe">Prediction Accuracy</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#25f268b3-8ad7-419b-a322-40566ffe1bfa">Transformer Model</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#a260fa1e-c4c9-485d-8a38-3eb075de7a3b">Implementation</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#bee09631-7e0f-44bf-8c8c-a63bf5999582">Multi-Task Transformer Model</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#cc5fac8a-7a38-49c7-a6dc-80a3bd03d1ab">Implementation</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#3f79e649-1b92-4b36-a743-97d2da9edd75">Multi-Task Transformer with User Buy Time Augmentation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#abb29175-bd82-4028-9c1a-e75e74759296">Implementation</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#89fa351d-da79-43fa-bba1-c4e09f5079c8">Augmented Multi-Task Transformer Model</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#6b93079a-3ec6-4394-82a4-333b3021aa40">Model Summary</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#3a14e224-c248-4490-8d8b-1f4651458062">Notebooks</a></div></nav><hr id="ede0e6a3-05c7-4dc2-99fe-d883f1c76dea"/><p id="1a58cb0c-18f4-4609-8c6d-62f339c6389c" class="">The objective is to predict each user’s purchasing feedback to nine exposed items, given this user’s click history, portrait features, and items’ features, which is similar to bundle recommendation. The special setting in this task is that the nine items are grouped into three sessions. The user can only unlock the subsequent session after he/she buys all three items in the current session. We want to predict whether a user would buy the nine exposed items or not.</p><figure id="8febff62-eb71-4ebd-848a-8f319645ba9d" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled.png"><img style="width:592px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled.png"/></a></figure><p id="f3010c2b-ba77-4117-83ac-7aa8c860727d" class="">The evaluation metric for this task is the Categorization Accuracy measure, which is defined as follows:</p><figure id="bd6c60b1-ad4f-4f81-a4e0-51fa26eee1c8" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext mathvariant="bold">accuracy</mtext><mo>=</mo><mfrac><mn>1</mn><mi>M</mi></mfrac><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>9</mn></munderover><mo stretchy="false">[</mo><msub><mi>y</mi><mrow><mi>u</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>u</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\textbf{accuracy} = \dfrac{1}{M}\sum_{u=1}^M\prod_{j=1}^9[y_{u,j}=\hat{y}_{u,j}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord textbf">accuracy</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.242113em;vertical-align:-1.4137769999999998em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011130000000004em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span></div></figure><p id="9bf6bc61-ddad-4422-960e-12e3732aa492" class="">Overall speaking, this task is challenging in two aspects.</p><ul id="b86c06c3-f16b-46f9-9db1-30fbe13a550b" class="bulleted-list"><li style="list-style-type:disc">Firstly, the nine exposed items are correlated and treated differently by the users. We cannot simply apply a single traditional recommendation method to predict each interaction independently.</li></ul><ul id="48be273f-b40f-4a23-8a49-f08b256e48ff" class="bulleted-list"><li style="list-style-type:disc">Secondly, with the given evaluation metric, it is required to correctly predict all of the nine interactions of a user, while partially correct predictions contribute nothing to the final score.</li></ul><h2 id="65b230ee-d706-4522-bca7-b25f9f706952" class="">Data Exploration</h2><h3 id="dbf4e5cf-7bbb-4709-bbad-2542ddb80bb0" class="">Overall data statistics</h3><p id="100b3b72-01bc-4b5c-a626-7a4fbac92faa" class="">In total, there are 381 items. There are 260,087 buying entries for training and 206,254 buying entries for testing. These entries are also accompanied by 10,435,798 and 8,357,719 clicking logs, respectively.</p><figure id="4670b00c-53d3-4227-9c9c-fae3f933380e" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%201.png"><img style="width:740px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%201.png"/></a></figure><h3 id="961f0891-87ac-40ec-86e2-278f4d44490e" class="">Click and buy statistics in different sessions</h3><p id="6afd027e-cf7b-4983-ba59-393d3ec8e22f" class="">It’s worth noticing that an item would only appear in its specific session. We can see that items in later sessions are with more types, and items with earlier sessions possess more clicks and buys. This is reasonable since users need to buy early items in order to unlock items (with higher prices) in the later sessions.</p><figure id="3f19a000-da79-4c5c-9476-1fa5c2397f8a" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%202.png"><img style="width:705px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%202.png"/></a></figure><h3 id="a9ff1b9c-11de-49c0-a0a7-359db16daa5b" class="">Buying behavior analysis</h3><figure id="0588f627-4c1a-4b95-88ff-f9a453d98ac6" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%203.png"><img style="width:680px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%203.png"/></a><figcaption>Histogram of the number of buys of each user.</figcaption></figure><p id="19f5d881-20c0-435d-a3fb-87266c6423f6" class="">We can classify users into four groups according to the number of items they have bought as follows,</p><ul id="d6fffc87-9bfa-48c2-97ad-d7b0b717580e" class="bulleted-list"><li style="list-style-type:disc">Group-0: 30,912 users who have bought 0 item.</li></ul><ul id="739bd06d-02af-4e8c-9172-79c97ae84d30" class="bulleted-list"><li style="list-style-type:disc">Group-1: 50,267 users who have bought 1∼3 items.</li></ul><ul id="a1e8f702-e31c-48b3-9620-c8b5c54d1bca" class="bulleted-list"><li style="list-style-type:disc">Group-2: 38,191 users who have bought 4∼6 items.</li></ul><ul id="4846c11b-bef0-43f5-91ce-7d029eb7a5e2" class="bulleted-list"><li style="list-style-type:disc">Group-3: 140,717 users who have bought 7∼9 items.</li></ul><p id="13891092-ee7b-4bd7-89c3-e30007f20ee3" class="">We can see that a decent population (Group-0) didn’t buy anything, the number of users who bought 4∼6 items (Group2) are the fewest, and a large portion of users (Group-3) chose to buy no less than seven items. This indicates an hourglass shape of user distribution. It’s also worth noticing that very few people buy three or six items.</p><p id="7f48ea1a-7770-4b15-b730-39198e49aa6a" class="">The main reason why a user buys three or six items might be to unlock and buy items in the next session.</p><h3 id="fbf1a9bb-70f1-4f61-b868-53699c4d128b" class="">Click behavior analysis</h3><p id="5172ae91-3757-4025-a11d-f965a59ccfc0" class="">There are 28,184 users who did not click anything. However, we do see that the majority of users are with a decent number of clicks.</p><figure id="d05535b3-86d3-45b5-9f9e-14062b95a2df" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%204.png"><img style="width:588px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%204.png"/></a><figcaption>Histogram of the number of clicks of each user.</figcaption></figure><h3 id="890e388e-9389-42cf-8060-d79d2ba18a79" class="">User portrait features</h3><figure id="6eabb7f0-72c2-4677-8f59-236eb050c92a" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%205.png"><img style="width:1250px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%205.png"/></a></figure><h3 id="23273250-6536-46e4-88dd-59726405a5bc" class="">Item features</h3><figure id="b114b6e7-b440-412f-9729-2d52e9bca28a" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%206.png"><img style="width:1212px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%206.png"/></a></figure><h3 id="e96d495e-92e9-4bcc-9b81-3dbee4c01ecb" class="">Data snapshots</h3><p id="a14cf97b-feb2-4378-bd01-a192a3ea96e2" class="">train.csv:</p><figure id="34d1acf9-b500-4c33-8c94-c4d825d1bde7" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%207.png"><img style="width:1241px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%207.png"/></a></figure><p id="ed1a21e8-35a4-4e3a-bb74-3df0d29e9dbf" class="">item_info.csv</p><figure id="9be99a29-1d47-4b15-ba2c-e1145653e77c" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%208.png"><img style="width:382px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%208.png"/></a></figure><p id="f70c5038-0c8d-48e2-bf3f-320a72e204b9" class="">track1_testset.csv</p><figure id="f209ebb5-240a-4972-8268-cf831abf02b1" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%209.png"><img style="width:1254px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%209.png"/></a></figure><p id="1298fa68-a4be-4362-abc8-947a45580b99" class="">track2_testset.csv</p><figure id="8e5466c9-a6b9-4326-a558-1ebadd12cb42" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2010.png"><img style="width:824px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2010.png"/></a></figure><h2 id="7100d80a-545b-4693-b331-867f276279f8" class="">Data Preprocessing</h2><h3 id="bc6c8cc9-a59c-4781-b7b3-5df616402e68" class="">Item Info Processing</h3><p id="0a19b4bd-8f3c-4a03-ba00-34e9056ca480" class="">Raw data head: </p><figure id="d11a4223-a4ac-4f1f-b39b-c8dfe287e710" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2011.png"><img style="width:321px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2011.png"/></a></figure><p id="0b4b800c-a983-4361-bc0e-53edcabba8af" class="">Processed data head:</p><figure id="98006ad4-8d9b-49d8-80c7-9f37610c75b1" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2012.png"><img style="width:456px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2012.png"/></a></figure><p id="94bc8403-64ac-41ab-86a0-b38a8629c1bc" class="">Transformation code:</p><pre id="8d71882f-e224-417d-8fa3-dc097fe8d995" class="code code-wrap"><code>item_info_dict = {}
for i in tqdm(range(df_item_info.shape[0])):
    item_id = df_item_info.at[i, &#x27;item_id&#x27;] 

    item_discrete = df_item_info.at[i, &#x27;item_vec&#x27;].split(&#x27;,&#x27;)[:3]
    item_cont = df_item_info.at[i, &#x27;item_vec&#x27;].split(&#x27;,&#x27;)[-2:]
    price = df_item_info.at[i, &#x27;price&#x27;] / 3000
    loc = df_item_info.at[i, &#x27;location&#x27;] - 1 # 0~2

    item_cont.append(price) # 2 + 1
    item_discrete.append(loc) # 3 + 1

    item_cont = [float(it) for it in item_cont]
    item_discrete = [int(it) for it in item_discrete]
    item_discrete[0] = item_discrete[0] - 1 # 1~4 -&gt; 0~3
    item_discrete[2] = item_discrete[2] - 1 # 1~2 -&gt; 0~1

    item_info_dict[int(item_id)] = {
        &#x27;cont&#x27;: np.array(item_cont, dtype=np.float64),
        &#x27;discrete&#x27;: np.array(item_discrete, dtype=np.int64),
    }</code></pre><h3 id="649e4771-7318-43b7-a71f-0dced86453ec" class="">Train data processing</h3><p id="b216a469-e064-4e34-810d-c832143983ee" class="">Raw data head: </p><figure id="cf14ffd7-7bdd-49a0-b78c-8f2f3d0b5265" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%207.png"><img style="width:1241px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%207.png"/></a></figure><p id="8ba3f7eb-f0e7-4975-b46d-8ad185fa30db" class="">Processed data head:</p><figure id="7732dbb1-2cc6-4df6-8c7b-463a3f683b66" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2013.png"><img style="width:715px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2013.png"/></a></figure><p id="5d9815f9-f7cb-4940-b5d3-eef7bc65b5b2" class="">Transformation code:</p><pre id="0e0a47cf-699b-4866-a8da-1017fce32750" class="code code-wrap"><code># trainset
train_samples = []
val_samples = []

# shuffle
# df_train = shuffle(df_train, random_state=2333).reset_index() # not shuffling - for this tutorial
total_num = int(df_train.shape[0])
num_train = int(total_num * 0.95)
num_val = total_num - num_train

for i in tqdm(range(total_num)):
    if df_train.at[i, &#x27;user_click_history&#x27;] == &#x27;0:0&#x27;:
        user_click_list = [0]
    else:
        user_click_list = df_train.at[i, &#x27;user_click_history&#x27;].split(&#x27;,&#x27;)
        user_click_list = [int(sample.split(&#x27;:&#x27;)[0]) for sample in user_click_list]
    num_user_click_history = len(user_click_list)
    tmp = np.zeros(400, dtype=np.int64)
    tmp[:len(user_click_list)] = user_click_list
    user_click_list = tmp
    
    exposed_items = [int(s) for s in df_train.at[i, &#x27;exposed_items&#x27;].split(&#x27;,&#x27;)]
    labels = [int(s) for s in df_train.at[i, &#x27;labels&#x27;].split(&#x27;,&#x27;)]

    user_portrait = [int(s) for s in df_train.at[i, &#x27;user_protrait&#x27;].split(&#x27;,&#x27;)]
    # portraitidx_to_idx_dict_list: list of 10 dict, int:int
    for j in range(10):
        user_portrait[j] = portraitidx_to_idx_dict_list[j][user_portrait[j]]
    for k in range(9):
        one_sample = {
            &#x27;user_click_list&#x27;: user_click_list,
            &#x27;num_user_click_history&#x27;: num_user_click_history,
            &#x27;user_portrait&#x27;: np.array(user_portrait, dtype=np.int64),
            &#x27;item_id&#x27;: exposed_items[k],
            &#x27;label&#x27;: labels[k]
        }
        if i &lt; num_train:
            train_samples.append(one_sample)
        else:
            val_samples.append(one_sample)</code></pre><h3 id="f814079b-d115-437e-b58c-59a1288ec477" class="">Test data processing</h3><p id="41224ab1-c31c-4bcf-b163-bf35112dde7d" class="">Raw data head:</p><figure id="fcbeceba-1369-4340-913a-03273b25eb30" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%209.png"><img style="width:1254px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%209.png"/></a></figure><p id="59576b0a-2daa-420f-858d-2b1ce7011832" class="">Processed data head:</p><figure id="41f04ecd-b004-4915-9155-53b33ce0f60e" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2014.png"><img style="width:540px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2014.png"/></a></figure><p id="afc8e2c4-dcb7-4b07-a148-24d687f3b753" class="">Transformation code:</p><pre id="aeebb5f0-ed3f-45c6-8957-3da53761569f" class="code code-wrap"><code># testset
test_samples = []

# shuffle
total_num = int(df_test1.shape[0])

for i in tqdm(range(total_num)):
    if df_test1.at[i, &#x27;user_click_history&#x27;] == &#x27;0:0&#x27;:
        user_click_list = [0]
    else:
        user_click_list = df_test1.at[i, &#x27;user_click_history&#x27;].split(&#x27;,&#x27;)
        user_click_list = [int(sample.split(&#x27;:&#x27;)[0]) for sample in user_click_list]
    num_user_click_history = len(user_click_list)
    tmp = np.zeros(400, dtype=np.int64)
    tmp[:len(user_click_list)] = user_click_list
    user_click_list = tmp
    
    exposed_items = [int(s) for s in df_test1.at[i, &#x27;exposed_items&#x27;].split(&#x27;,&#x27;)]
    labels = [int(s) for s in df_test1.at[i, &#x27;labels&#x27;].split(&#x27;,&#x27;)]

    user_portrait = [int(s) for s in df_test1.at[i, &#x27;user_protrait&#x27;].split(&#x27;,&#x27;)]
    # portraitidx_to_idx_dict_list: list of 10 dict, int:int
    for j in range(10):
        user_portrait[j] = portraitidx_to_idx_dict_list[j][user_portrait[j]]
    for k in range(9):
        one_sample = {
            &#x27;user_click_list&#x27;: user_click_list,
            &#x27;num_user_click_history&#x27;: num_user_click_history,
            &#x27;user_portrait&#x27;: np.array(user_portrait, dtype=np.int64),
            &#x27;item_id&#x27;: exposed_items[k],
        }
        test_samples.append(one_sample)</code></pre><h3 id="1074d92c-5890-4cf6-a682-2f597a3ccbce" class="">Item discretization</h3><p id="50085872-3a0e-4eac-a25d-f62c8cddd27d" class="">Raw data head: </p><figure id="5c9c0418-ac7d-4792-a360-9f26ab2d3ae3" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%207.png"><img style="width:1241px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%207.png"/></a></figure><p id="9d3f8d22-316f-4a4a-8d96-3481423ed8f7" class="">Processed data head:</p><figure id="d8d56da5-f477-43db-b514-c0115050f46d" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2015.png"><img style="width:226px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2015.png"/></a></figure><p id="3c44bf90-a6e9-4ef9-8500-db882c1cedb6" class="">Transformation code:</p><pre id="a806a47d-8262-49dc-b10b-7ac241ecd5bd" class="code code-wrap"><code>def load_item_info_turn_cont_to_discrete(
    data_path=&#x27;/content/&#x27;
):

    # item info
    df_item_info = pd.read_parquet(f&#x27;{data_path}/item_info.parquet.snappy&#x27;)

    num_items = 381+1 # 0 means no item; normal items start from 1
    num_features = (3+1) + (2+1)
    item_features = np.zeros((num_items, num_features)).astype(np.int64)

    for i in tqdm(range(num_items - 1)):
        item_id = df_item_info.at[i, &#x27;item_id&#x27;]
        # discrete
        item_discrete = df_item_info.at[i, &#x27;item_vec&#x27;].split(&#x27;,&#x27;)[:3]
        loc = df_item_info.at[i, &#x27;location&#x27;] - 1 # 0~2
        item_discrete.append(loc)
        item_discrete = [int(it) for it in item_discrete]
        item_discrete[0] = item_discrete[0] - 1 # 1~4 -&gt; 0~3
        item_discrete[2] = item_discrete[2] - 1 # 1~2 -&gt; 0~1

        # cont
        item_cont = df_item_info.at[i, &#x27;item_vec&#x27;].split(&#x27;,&#x27;)[-2:]
        price = df_item_info.at[i, &#x27;price&#x27;]
        item_cont.append(price)
        item_cont = [float(it) for it in item_cont]

        item_cont1 = cont1_to_discrete(item_cont[0])
        item_cont2 = cont2_to_discrete(item_cont[1])
        item_cont3 = cont3_to_discrete(item_cont[2])

        # agg
        item_discrete.append(item_cont1)
        item_discrete.append(item_cont2)
        item_discrete.append(item_cont3)

        item_total_feat = np.array(item_discrete, dtype=np.int64)
        item_features[item_id] = item_total_feat
    
    # change 0 item to no-feature (last idx of each feature + 1)
    last_idx = np.max(item_features, axis=0)
    item_features[0] = last_idx + 1

    return item_features</code></pre><h3 id="60f5abf1-0aaf-45b7-8624-03bbd4f6db76" class="">Training dataset preparation (torch Tensor Dataset format)</h3><p id="0a16552b-1890-430e-a256-f3bb12e42568" class="">Prepared dataset head:</p><figure id="b843acba-4aeb-4d79-a94c-d24109c2ae3e" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2016.png"><img style="width:732px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2016.png"/></a></figure><p id="3c93384f-0795-4df7-a36a-9ac32c1d7b33" class="">Transformation code:</p><pre id="2e6e5141-5075-477a-9c01-2ccab70eff0d" class="code code-wrap"><code>class BigDataCupDataset(torch.utils.data.Dataset):
    def __init__(self, 
                 item_info_dict,
                 database
                ):
        super().__init__()
        self.item_info_dict = item_info_dict
        self.database = database

    def __len__(self, ):
        return len(self.database)

    def __getitem__(self, idx):
        one_sample = self.database[idx]
        user_click_history = one_sample[&#x27;user_click_list&#x27;]
        num_user_click_history = one_sample[&#x27;num_user_click_history&#x27;]
        user_discrete_feature = one_sample[&#x27;user_portrait&#x27;]
        item_id = one_sample[&#x27;item_id&#x27;]
        item_discrete_feature = self.item_info_dict[item_id][&#x27;discrete&#x27;]
        item_cont_feature = self.item_info_dict[item_id][&#x27;cont&#x27;]
        label = one_sample[&#x27;label&#x27;]

        # print(num_user_click_history)

        user_click_history = torch.IntTensor(user_click_history)
        num_user_click_history = torch.IntTensor([num_user_click_history])
        user_discrete_feature = torch.IntTensor(user_discrete_feature)
        item_id = torch.IntTensor([item_id])
        item_discrete_feature = torch.IntTensor(item_discrete_feature)
        item_cont_feature = torch.FloatTensor(item_cont_feature)
        label = torch.IntTensor([label])

        # print(num_user_click_history)

        return user_click_history, num_user_click_history, user_discrete_feature, \
               item_id, item_discrete_feature, item_cont_feature, label</code></pre><h3 id="bacac4b4-edc3-49c9-b6d4-69da1cdca1c3" class="">Testing dataset preparation (torch Tensor Dataset format)</h3><p id="aa956e29-ad05-43d3-9f8f-f0748f344f88" class="">Prepared dataset head:</p><figure id="3aeaf13e-6c85-43ba-90da-1350c22eb6d4" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2017.png"><img style="width:731px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2017.png"/></a></figure><p id="a182627b-c828-4aeb-b340-b87dc4c5bc5e" class="">Transformation code:</p><pre id="ff0266d5-80dd-4c77-a1d2-0c71384c8730" class="code code-wrap"><code>class BigDataCupTestDataset(torch.utils.data.Dataset):
    def __init__(self, 
                 item_info_dict,
                 database
                ):
        super().__init__()
        self.item_info_dict = item_info_dict
        self.database = database

    def __len__(self, ):
        return len(self.database)

    def __getitem__(self, idx):
        one_sample = self.database[idx]
        user_click_history = one_sample[&#x27;user_click_list&#x27;]
        num_user_click_history = one_sample[&#x27;num_user_click_history&#x27;]
        user_discrete_feature = one_sample[&#x27;user_portrait&#x27;]
        item_id = one_sample[&#x27;item_id&#x27;]
        item_discrete_feature = self.item_info_dict[item_id][&#x27;discrete&#x27;]
        item_cont_feature = self.item_info_dict[item_id][&#x27;cont&#x27;]

        user_click_history = torch.IntTensor(user_click_history)
        num_user_click_history = torch.IntTensor([num_user_click_history])
        user_discrete_feature = torch.IntTensor(user_discrete_feature)
        item_id = torch.IntTensor([item_id])
        item_discrete_feature = torch.IntTensor(item_discrete_feature)
        item_cont_feature = torch.FloatTensor(item_cont_feature)

        return user_click_history, num_user_click_history, user_discrete_feature, \
               item_id, item_discrete_feature, item_cont_feature</code></pre><h2 id="ee8713dc-8cf7-4907-b6c1-d5f947c8fe07" class="">MLP Model</h2><p id="ad876f5a-e580-41d5-999f-0cc86259e8ba" class="">We start with a very simple basic network. The network takes the following inputs: user profile features, user clicked items’ id and features, nine exposed target items’ id and features. These inputs are processed by their corresponding embedding layers, and further fed to an MLP module. Then the network predicts whether the user will buy the nine exposed target items. We propose this framework since the nine items’ labels are correlated. For example, users might buy all of the first six items, only to unlock and buy subsequent items. Therefore, it is not suitable to predict the nine feedback independently, and we need to ensure the network is able to predict nine feedback simultaneously. The training of the model is supervised by a vanilla binary cross entropy (BCE) loss on each item respectively.</p><h3 id="39d0c9d8-34a5-43bf-b007-5f407d4c4696" class="">Architecture</h3><figure id="51b67307-6c0c-44b2-8c72-fe0492681d9b" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2018.png"><img style="width:601px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2018.png"/></a></figure><h3 id="67691604-5511-4827-bdae-010f641d6ae5" class="">Implementation</h3><pre id="be570e54-7f77-47ea-b990-6d22eaeeeb6a" class="code code-wrap"><code>class VanillaBaseModel(nn.Module):
    def __init__(self, 
                 num_items,
                 dim_item_emb=64,
                 dim_item_discrete_feature_emb=16,
                 dim_user_discrete_feature_emb=16,
                ):
        super().__init__()
        self.NUM_ITEM_DISCRETE_FEATURE = 3 + 1 # item_vec3 + location1
        self.NUM_ITEM_CONT_FEATURE = 2 + 1 # item_vec2 + price1
        self.NUM_USER_DISCRETE_FEATURE = 10
        self.dim_item_emb = dim_item_emb

        self.item_emb = nn.Embedding(num_items + 1, dim_item_emb) # num_items + 1
        
        # item discrete feature
        self.item_discrete_feature_emb_list = nn.ModuleList()
        num_unique_value_list = [4, 10, 2, 3]
        for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
            num_unique_value = num_unique_value_list[i]
            self.item_discrete_feature_emb_list.append(
                nn.Embedding(num_unique_value, dim_item_discrete_feature_emb)
            )
        
        # user discrete feature
        self.user_discrete_feature_emb_list = nn.ModuleList()
        num_unique_value_list = [3, 1430, 20, 10, 198, 52, 3, 13, 2, 2347]
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            num_unique_value = num_unique_value_list[i]
            self.user_discrete_feature_emb_list.append(
                nn.Embedding(num_unique_value, dim_user_discrete_feature_emb)
            )

        # backbone
        self.backbone = nn.Sequential(
            nn.Linear(dim_item_emb + # user_click_history
                      self.NUM_ITEM_DISCRETE_FEATURE * dim_item_discrete_feature_emb + 
                      self.NUM_ITEM_CONT_FEATURE + 
                      self.NUM_USER_DISCRETE_FEATURE * dim_user_discrete_feature_emb +
                      dim_item_emb, 200), 
            nn.Dropout(0.1),
            nn.PReLU(),
            nn.LayerNorm(200),
            nn.Linear(200, 80),
            nn.PReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(80),
            nn.Linear(80, 1)
        )


    def forward(self,
                user_click_history,
                num_user_click_history,
                user_discrete_feature,
                item_id,
                item_discrete_feature,
                item_cont_feature
                ):
        &quot;&quot;&quot;
        user_click_history: [N, 300]
        num_user_click_history: [N, 1]
        user_discrete_feature: [N, 10]
        item_id: [N, 1]
        item_discrete_feature: [N, 3 + 1], item_vec3 + location1
        item_cont_feature: [N, 2 + 1], item_vec2 + price1
        &quot;&quot;&quot;

        batch_size = user_click_history.size()[0]
    
        # user click history emb
        tmp = self.item_emb(user_click_history) # [N, 300] -&gt; [N, 300, dim_item_emb]
        user_click_history_emb = torch.zeros((batch_size, self.dim_item_emb))
        for i in range(batch_size):
            #print(num_user_click_history[i])
            aa = tmp[i, :num_user_click_history[i], :] # [N, D]
            #print(aa.shape)
            a = torch.mean(aa, dim=0) # [N, d] -&gt; [1, d]
            #print(a.shape)
            #print(user_click_history_emb.shape)
            user_click_history_emb[i] = a

        ## User Profile Features
        # user discrete feature, 10 features
        tmp = []
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            tmp.append(
                self.user_discrete_feature_emb_list[i](user_discrete_feature[:, i]) # [N, dim_user_discrete_feature_emb]
            )
        user_discrete_feature_emb = torch.cat(tmp, dim=1)

        ## Item
        # item discrete feature, 3 features
        tmp = []
        for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
            # print(i)
            # print(item_discrete_feature[:, i])
            tmp.append(
                self.item_discrete_feature_emb_list[i](item_discrete_feature[:, i]) # [N, dim_user_discrete_feature_emb]
            )
        item_discrete_feature_emb = torch.cat(tmp, dim=1)
        # item emb
        item_emb = self.item_emb(item_id)
        item_emb = torch.squeeze(item_emb)

        ## all emb
        #print(user_click_history_emb.size())
        #print(user_discrete_feature_emb.size())
        #print(item_discrete_feature_emb.size())
        #print(item_cont_feature.size())
        #print(item_emb.size())

        all_emb = torch.cat([user_click_history_emb, 
                             user_discrete_feature_emb,
                             item_discrete_feature_emb,
                             item_cont_feature,
                             item_emb,
                            ], dim=1) # [N, D]
        
        out = self.backbone(all_emb) # [N, 1]
        return out</code></pre><p id="f06a4967-d07f-466f-8f97-dd8d99f59247" class="">A dry run of this model would give output like this:</p><figure id="6a404a5c-c9c4-4326-be4e-a25f99d4eff5" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2019.png"><img style="width:779px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2019.png"/></a></figure><h3 id="5e350009-2f1b-4182-89f4-094bd1ce5b12" class="">Training and Validation Output</h3><pre id="8e03aee6-8a58-4876-8b66-82bf48c58bf8" class="code code-wrap"><code>----- VAL -----
- acc: tensor(0.7849)
- real acc: 0.27197231833910035
- real rule acc: 0.2829680891964629
- real rule2 acc: 0.2965782391387928
----- TRAIN -----
[2, 68002] loss: 455.965
- acc: tensor(0.7854)
13005it [00:27, 477.71it/s]
----- VAL -----
- acc: tensor(0.7829)
- real acc: 0.27935409457900806
- real rule acc: 0.2901960784313726
- real rule2 acc: 0.2998077662437524
----- TRAIN -----
[2, 69002] loss: 452.265
- acc: tensor(0.7887)
13005it [00:27, 467.48it/s]
----- VAL -----
- acc: tensor(0.7852)
- real acc: 0.26935793925413304
- real rule acc: 0.28104575163398693
- real rule2 acc: 0.294963475586313
Finished Training</code></pre><h2 id="4830e045-b2c5-472a-998f-653fce275b18" class="">Session Prediction</h2><p id="29ac630a-e21a-4a9f-b114-7bc1bb6b1f47" class="">To better model users’ buying behaviors, we classify the nine exposed items into four types (weak positive, strong positive, strong negative, weak negative) as shown in the following figure.</p><figure id="b18077d9-22b5-4780-a40d-8e7b93fcb180" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2020.png"><img style="width:592px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2020.png"/></a></figure><h3 id="79ffad49-ea1a-47fc-bc0e-78bc9cddaa0d" class="">Architecture</h3><figure id="a4bc31e4-ba22-421d-88cd-b4b355f1f73b" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2021.png"><img style="width:601px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2021.png"/></a></figure><h3 id="4406796d-b2cb-40bb-8edb-0835a845affb" class="">Implementation</h3><pre id="8bda9a59-9d2a-46d7-aa5b-5d8358661e6e" class="code code-wrap"><code># backbone
self.backbone = nn.Sequential(
    nn.Linear(
        # user_click_history
        self.dim_item_emb + self.NUM_ITEM_DISCRETE_FEATURE * self.dim_item_discrete_feature_emb + self.NUM_ITEM_CONT_FEATURE + 
        # nine items
        9 * (self.dim_item_emb + self.NUM_ITEM_DISCRETE_FEATURE * self.dim_item_discrete_feature_emb + self.NUM_ITEM_CONT_FEATURE) +
        # user
        self.NUM_USER_DISCRETE_FEATURE * self.dim_user_discrete_feature_emb
        , 400), 
    nn.PReLU(),
    nn.Linear(400, 200),
    nn.PReLU()
)

# session prediction head
self.session_prediction_head = nn.Sequential(
    nn.Linear(200, 80),
    nn.PReLU(),
    nn.Linear(80, 20),
    nn.PReLU(),
    nn.Linear(20, 4)
)

# buy prediction head
self.buy_prediction_head = nn.Sequential(
    nn.Linear(200, 80),
    nn.PReLU(),
    nn.Linear(80, 20),
    nn.PReLU(),
    nn.Linear(20, 9)
)</code></pre><p id="e11171fa-166a-4c50-a05a-b9c3290b7773" class="">A dry run of this model would give output like this:</p><figure id="35f7db3b-a241-435c-83cb-2cc42d7ad296" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2022.png"><img style="width:777px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2022.png"/></a></figure><p id="03d7d01e-dee2-4f47-885e-c657e4c2903d" class="">Output:</p><ol type="1" id="5c048a1e-a3b9-42a3-8ce3-ce3ad419adf2" class="numbered-list" start="1"><li>The user purchased in the session (purchased 0-3, 4-6, 7-9, three types of sessions)</li></ol><ol type="1" id="2edc31c6-409e-4155-8eb0-1544213186a3" class="numbered-list" start="2"><li>Whether the user bought these 9 products</li></ol><h3 id="2f18eb32-9502-495e-b551-25e8fa138015" class="">Training and Validation Output</h3><pre id="11bd352b-d8f4-4bf6-9ed6-e2d1c0ad6594" class="code code-wrap"><code>----- VAL -----
- sess acc: 0.6585239085239085
- buy acc1: tensor(0.8087)
- buy real acc1: 0.3279212341712342
- buy acc2: tensor(0.8059)
- buy real acc2: 0.3299529862029862
- buy acc rule2: tensor(0.8087)
- buy real acc rule2: 0.3284587034587035
valid sess cnt: tensor(1589) tensor(2513) tensor(1935) tensor(6968)
7051
7101
7151
7201
7251
7301
7351
7401
7451
7501
----- TRAIN -----
[2,  7502] sess loss: 0.807
[2,  7502] buy loss: 0.388
- sess acc: 0.6786875
- buy acc1: tensor(0.8213)
- buy real acc1: 0.3445625
- buy acc2: tensor(0.8197)
- buy real acc2: 0.3479375
- buy acc rule2: tensor(0.8214)
- buy real acc rule2: 0.345625
- train sess cnt: tensor(1885) tensor(3096) tensor(2377) tensor(8642)
407it [00:07, 57.13it/s]
----- VAL -----
- sess acc: 0.6593685031185031
- buy acc1: tensor(0.8088)
- buy real acc1: 0.3343354280854281
- buy acc2: tensor(0.8062)
- buy real acc2: 0.3364439614439615
- buy acc rule2: tensor(0.8087)
- buy real acc rule2: 0.3349496786996787
valid sess cnt: tensor(1589) tensor(2513) tensor(1935) tensor(6968)
7551
7601
7651
7701
Finished Training</code></pre><h3 id="302441c8-fcc7-4998-b16a-d658d30b85a6" class="">Prediction Accuracy</h3><p id="d085894f-c86c-4f78-99ae-119fcc367900" class="">Session prediction:</p><div id="86c53687-c5ab-424c-8c47-32bf5484989f" class="column-list"><div id="a53b074a-18b5-492d-b46a-7a9be074e45c" style="width:50%" class="column"><figure id="659d5e1a-c818-4684-b8e5-36f0ac5709e1" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2023.png"><img style="width:318px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2023.png"/></a><figcaption>Session prediction - ground-truth vs prediction</figcaption></figure></div><div id="5818e9a4-5069-45d3-a79e-115a1e05ddbf" style="width:50%" class="column"><figure id="9ea007da-6118-4377-831d-f59db3db9e96" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2024.png"><img style="width:306px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2024.png"/></a><figcaption>Session prediction - ground-truth vs prediction (in percentage)</figcaption></figure></div></div><p id="1b3b67cb-d142-4d59-91b0-fdaa3f5122c6" class="">Item prediction:</p><div id="599bfd2f-4062-4c7b-905c-735c5f9783cf" class="column-list"><div id="25f45107-d417-44f9-aa9f-70b35904dd01" style="width:50%" class="column"><figure id="46510356-4157-4d39-97f5-92757281305e" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2025.png"><img style="width:319px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2025.png"/></a><figcaption>Item prediction - ground-truth vs prediction</figcaption></figure></div><div id="4e7e73e4-1276-450a-b477-1a5d3d7b90ec" style="width:50%" class="column"><figure id="d9adbd2b-3ed9-49b4-9e76-438a177562fb" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2026.png"><img style="width:306px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2026.png"/></a><figcaption>Item prediction - ground-truth vs prediction (in percentage)</figcaption></figure></div></div><h2 id="24c875ee-48bd-4c81-942f-a5c5435b2c22" class="">Feature Layer Augmentation</h2><p id="618696c6-5307-4af4-bd44-87072497df79" class="">The feature layer of item is added, because when history was originally considered, all feats were directly averaged, which is unreasonable for features such as price.</p><h3 id="d1ff7273-7b0a-400f-982c-24f159494c04" class="">Implementation</h3><pre id="41c2f40f-944a-47a3-960d-253859f96b80" class="code code-wrap"><code># backbone
self.backbone = nn.Sequential(
    nn.Linear(
        # user_click_history
        self.dim_item_extraction + 
        # nine items
        9 * self.dim_item_extraction +
        # user
        self.NUM_USER_DISCRETE_FEATURE * self.dim_user_discrete_feature_emb
        , 400), 
    nn.PReLU(),
    nn.Linear(400, 200),
    nn.PReLU()
)

# item feature extraction
self.item_feature_extraction = nn.Sequential(
    nn.Linear(self.dim_item_emb 
                + self.NUM_ITEM_DISCRETE_FEATURE * self.dim_item_discrete_feature_emb 
                + self.NUM_ITEM_CONT_FEATURE, 
                self.dim_item_extraction),
    nn.PReLU()
)

# session prediction head
self.session_prediction_head = nn.Sequential(
    nn.Linear(200, 80),
    nn.PReLU(),
    nn.Linear(80, 20),
    nn.PReLU(),
    nn.Linear(20, 4)
)

# buy prediction head
self.buy_prediction_head = nn.Sequential(
    nn.Linear(200, 80),
    nn.PReLU(),
    nn.Linear(80, 20),
    nn.PReLU(),
    nn.Linear(20, 9)
)</code></pre><h3 id="9cc3db37-e8ac-4827-b604-a6d6bb2d252d" class="">Performance</h3><div id="9d0a3a27-ce3b-417f-99a2-a01c9b137ffe" class="column-list"><div id="b85095a2-f789-497d-ae3b-32ef5ddf63d5" style="width:50%" class="column"><figure id="de33ecc5-8bea-4465-b35c-c2c7167cd2a6" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2027.png"><img style="width:355px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2027.png"/></a></figure></div><div id="617f5e94-5d9b-41c8-be51-bb9d4a7b3d1e" style="width:50%" class="column"><figure id="039f097f-78d2-4eab-bede-eee938c8c2bf" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2028.png"><img style="width:353px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2028.png"/></a></figure></div></div><h3 id="1a6db28a-1c80-4dbd-95dd-d23b1a04affe" class="">Prediction Accuracy</h3><p id="275fe2d0-eb1d-4151-8645-82c89419fb4d" class="">Session prediction:</p><div id="210ffc1d-e386-471f-95ac-c5ae426f346a" class="column-list"><div id="d42cdc7d-293c-43a0-a9cb-8f975addd401" style="width:50%" class="column"><figure id="e6ad696c-6783-417f-b10c-396ecc9ad1b5" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2029.png"><img style="width:318px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2029.png"/></a><figcaption>Session prediction - ground-truth vs prediction</figcaption></figure></div><div id="fde58706-96e6-413a-8991-37dd42f52d7c" style="width:50%" class="column"><figure id="7df0edf7-d37d-4cb6-8c78-1b126f2f33de" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2030.png"><img style="width:306px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2030.png"/></a><figcaption>Session prediction - ground-truth vs prediction (in percentage)</figcaption></figure></div></div><p id="4bd039b6-596a-4266-9b16-53315fd9e27f" class="">Item prediction:</p><div id="0abf9079-ba18-4422-8404-35d12452bd89" class="column-list"><div id="771ecefb-c5c9-4277-ae95-fa47f878ed3a" style="width:50%" class="column"><figure id="7c736a32-b89e-49d6-9bed-d478d30ed28e" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2031.png"><img style="width:319px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2031.png"/></a><figcaption>Item prediction - ground-truth vs prediction</figcaption></figure></div><div id="ffc73c92-8a2f-4c06-8f8b-f5a51a7c5b2a" style="width:50%" class="column"><figure id="11d54187-5cdb-4158-ad8b-d61df12ec823" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2032.png"><img style="width:306px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2032.png"/></a><figcaption>Item prediction - ground-truth vs prediction (in percentage)</figcaption></figure></div></div><h2 id="25f268b3-8ad7-419b-a322-40566ffe1bfa" class="">Transformer Model</h2><p id="c72b7a7c-569c-416d-947d-46cf06c066ef" class="">Instead of simple MLPs, we switch the backbone part into a transformer, as their self-attention mechanism is proved to be effective on capturing inter-relations between different features.</p><h3 id="a260fa1e-c4c9-485d-8a38-3eb075de7a3b" class="">Implementation</h3><pre id="a3894d3a-d066-4267-9056-26f0cadc24cc" class="code code-wrap"><code>class MultiHeadSelfAttention(nn.Module):
    def __init__(self, 
                 hidden_size,
                 qkv_size, 
                 num_heads, 
                 dropout_ratio=0.
                ):
        super().__init__()
        self.n = num_heads
        self.d = qkv_size
        self.D = hidden_size

        self.scale = self.d ** -0.5
        self.to_qkv = nn.Linear(self.D, self.n * self.d * 3, bias=False)
        self.attend = nn.Softmax(dim=-1)
        self.to_out = nn.Sequential(
            nn.Linear(self.n * self.d, self.D),
            nn.Dropout(dropout_ratio)
        )
        
    def forward(self, x):
        &quot;&quot;&quot;
        x: BND
        output: BND
        &quot;&quot;&quot;
        B, N, D = x.shape
        
        # get qkv
        qkv_agg = self.to_qkv(x) # BND -&gt; BN(num_heads*qkv_size*3)
        qkv_agg = qkv_agg.chunk(3, dim=-1) # BND -&gt; 3 * [BN(num_heads*qkv_size)]
        q = einops.rearrange(qkv_agg[0], &#x27;B N (n d) -&gt; B n N d&#x27;, n=self.n)
        k = einops.rearrange(qkv_agg[1], &#x27;B N (n d) -&gt; B n N d&#x27;, n=self.n)
        v = einops.rearrange(qkv_agg[2], &#x27;B N (n d) -&gt; B n N d&#x27;, n=self.n)

        # calc self attention 
        dots = torch.einsum(&#x27;Bnid, Bnjd -&gt; Bnij&#x27;, q, k)     # BnNd, BnNd -&gt; BnNN
        attn = self.attend(dots * self.scale)
        out = torch.einsum(&#x27;BnNj, Bnjd -&gt; BnNd&#x27;, attn, v)   # BnNN, BnNd -&gt; BnNd
        out = einops.rearrange(out, &#x27;B n N d -&gt; B N (n d)&#x27;) # BnNd -&gt; BN(nd) = BND

        # aggregate multihead
        out = self.to_out(out)
        
        return out


class FeedForwardNetwork(nn.Module):
    def __init__(self, 
                 hidden_size,
                 mlp_size,
                 dropout_ratio
                ):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(hidden_size, mlp_size),
            nn.GELU(),
            nn.Dropout(dropout_ratio),
            nn.Linear(mlp_size, hidden_size),
            nn.Dropout(dropout_ratio)
        )

    def forward(self, x):
        &quot;&quot;&quot;
        x: BND
        output: BND
        &quot;&quot;&quot;
        return self.model(x)


class SessionPredictionItemFeatTransformer(nn.Module):
    def __init__(self, 
                 num_items=381,
                 hidden_size=128,
                 num_layers=3, 
                 mlp_size=64, # normally = 4 * hidden_size
                 qkv_size=32, # normally = 64 = hidden_size / num_heads
                 num_heads=4, 
                 msa_dropout_ratio=0.1, 
                 ffn_dropout_ratio=0.1, 
                 device=&#x27;cpu&#x27;
                ):

        super().__init__()
        self.device = device
        self.num_items = num_items
        self.NUM_ITEM_DISCRETE_FEATURE = 3+1 + 2+1 # item_vec3+location1 + item_vec2+price1
        self.NUM_USER_DISCRETE_FEATURE = 10
        self.hidden_size = hidden_size
        self.N = 1 + self.NUM_ITEM_DISCRETE_FEATURE + \
                 9 * (1 + self.NUM_ITEM_DISCRETE_FEATURE) + \
                 self.NUM_USER_DISCRETE_FEATURE

        # item emb
        self.item_emb = nn.Embedding(self.num_items + 1, self.hidden_size)

        # item discrete feature
        self.item_discrete_feature_emb_list = nn.ModuleList()
        num_unique_value_list = [4+1, 10+1, 2+1, 3+1, 5+1, 10+1, 11+1] # [4, 10, 2, 3]
        for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
            num_unique_value = num_unique_value_list[i]
            self.item_discrete_feature_emb_list.append(
                nn.Embedding(num_unique_value, self.hidden_size)
            )
        
        # user discrete feature
        self.user_discrete_feature_emb_list = nn.ModuleList()
        num_unique_value_list = [4, 1364, 21, 11, 196, 50, 4, 12, 3, 2165] # (already add 1 for features in test but not in train)
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            num_unique_value = num_unique_value_list[i]
            self.user_discrete_feature_emb_list.append(
                nn.Embedding(num_unique_value, self.hidden_size)
            )
        
        # position emb
        self.position_emb = nn.Parameter(torch.randn(1, self.N, self.hidden_size))

        # transformer layers
        self.transformer_layers = nn.ModuleList([])
        for _ in range(num_layers):
            self.transformer_layers.append(nn.ModuleList([
                nn.Sequential( # MSA(LN(x))
                    nn.LayerNorm(self.hidden_size),
                    MultiHeadSelfAttention(self.hidden_size, qkv_size, num_heads, msa_dropout_ratio),
                ),
                nn.Sequential( # MLPs(LN(x))
                    nn.LayerNorm(self.hidden_size),
                    FeedForwardNetwork(self.hidden_size, mlp_size, ffn_dropout_ratio)
                )
            ]))

        # session prediction head
        self.session_prediction_head = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.PReLU(),
            nn.Linear(64, 4)
        )

        # buy prediction head
        self.buy_prediction_head = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.PReLU(),
            nn.Linear(64, 9)
        )

    def get_item_emb_attr(self, 
                          item_id, 
                          item_discrete_feature):
        &quot;&quot;&quot;
        param:
            item_id:               [B, 9] 
            item_discrete_feature: [B, 9, NUM_USER_DISCRETE_FEATURE]
        return: 
            emb_attr:
                [B(batchsize), 9, N(num_feat=1+7), D(hiddendim)]
        note: 
            above, 9 can be an arbitrary number, e.g. 400
        &quot;&quot;&quot;
        tmp = []
        # item emb
        item_emb = self.item_emb(item_id) # [B, 9, D]
        tmp.append(torch.unsqueeze(item_emb, 2)) # [B, 9, 1, D]
        # item discrete feature emb
        for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
            a = self.item_discrete_feature_emb_list[i](item_discrete_feature[:, :, i]) # [B, 9, D]
            tmp.append(torch.unsqueeze(a, 2)) # [B, 9, 1, D]
        # cat to [B, 9, N, D]
        return torch.cat(tmp, dim=2) # [B, 9, 8, D]

    def forward(self,
                user_click_history, user_click_history_discrete_feature, num_user_click_history,
                item_id, item_discrete_feature,
                user_discrete_feature,
                ):
        &quot;&quot;&quot;
            user_click_history: [N, 400]
            user_click_history_discrete_feature: [N, 400, 3+1 + 2+1]
            num_user_click_history: [N, 1]
            item_id: [N, 9]
            item_discrete_feature: [N, 9, 3+1 + 2+1] item_vec3 + location1 + item_vec2 + price1
            user_discrete_feature: [B, 10]
        &quot;&quot;&quot;

        batch_size = user_click_history.size()[0]

        user_click_history_emb = torch.zeros( # [B, 8, D]
            (batch_size, 1 + self.NUM_ITEM_DISCRETE_FEATURE, self.hidden_size)
        ).to(self.device)
        assert 1 + self.NUM_ITEM_DISCRETE_FEATURE == 8
        tmp = self.get_item_emb_attr(user_click_history, user_click_history_discrete_feature) # [B, 400, 8, D]
        for i in range(batch_size):
            aa = tmp[i, :num_user_click_history[i], :, :] # [B, 400, 8, D] -&gt; [400-, 8, D]
            a = torch.mean(aa, dim=0) # [400-, 8, D] -&gt; [8, D]
            user_click_history_emb[i] = a
        
        nine_item_emb = self.get_item_emb_attr(item_id, item_discrete_feature) # [B, 9, 8, D]
        nine_item_emb = einops.rearrange(nine_item_emb, &#x27;B n N D -&gt; B (n N) D&#x27;) # [B, 9*8, D]

        tmp = []
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            a = self.user_discrete_feature_emb_list[i](user_discrete_feature[:, i]) # [B, D]
            tmp.append(torch.unsqueeze(a, 1)) # [B, 1, D]
        user_discrete_feature_emb = torch.cat(tmp, dim=1) # [B, 10, D]

        # concat all emb
        z0 = torch.cat([user_click_history_emb,     # [B, 8, D]
                        nine_item_emb,              # [B, 9*8, D]
                        user_discrete_feature_emb,  # [B, 10, D]
                        ], dim=1) # [B, N, D]

        position_embs = einops.repeat(self.position_emb, &#x27;() N D -&gt; B N D&#x27;, B=batch_size)
        z0 = z0 + position_embs

        # transformer
        zl = z0
        for transformer_layer in self.transformer_layers:
            zl = zl + transformer_layer[0](zl) # MSA(LN(x))
            zl = zl + transformer_layer[1](zl) # MLPs(LN(x))

        # global average pooling
        zl = einops.reduce(zl, &#x27;B N D -&gt; B D&#x27;, reduction=&#x27;mean&#x27;)

        # head
        session_pred = self.session_prediction_head(zl)
        buy_pred = self.buy_prediction_head(zl)

        return session_pred, buy_pred # [B, 4], [B, 9]</code></pre><p id="fbdfff4a-7e08-4f3f-b5a4-9136c7f48d82" class="">A dry run of this model would give output like this:</p><figure id="fc3df40e-add6-4069-97ca-750c88216b76" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2033.png"><img style="width:760px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2033.png"/></a></figure><p id="ca17eab8-85df-44e6-90c3-dd282e2ac52c" class="">Output:</p><ol type="1" id="11bde85e-57b5-43ea-87eb-05775d067c70" class="numbered-list" start="1"><li>The user purchased in the session (purchased 0-3, 4-6, 7-9, three types of sessions)</li></ol><ol type="1" id="f9fee97e-6757-4daf-b7d3-0dc6ceee6670" class="numbered-list" start="2"><li>Whether the user bought these 9 products</li></ol><h2 id="bee09631-7e0f-44bf-8c8c-a63bf5999582" class="">Multi-Task Transformer Model</h2><p id="ad648b3e-8d05-4cd9-ae49-a5d89fdb4e78" class="">Apart from the buy prediction network, we use another click prediction auxiliary network to assist the learning procedure. Note that the two networks share the same embedding layers. The click prediction network takes the following inputs: user profile features, the previously clicked items’ id and features, target items’ id, and features. It is trained to predict whether the user will click the target item or not.</p><figure id="56f5bd7e-3b90-4da9-a8ab-de4193697ec9" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2034.png"><img style="width:1321px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2034.png"/></a></figure><h3 id="cc5fac8a-7a38-49c7-a6dc-80a3bd03d1ab" class="">Implementation</h3><pre id="f29813fb-409d-49d2-879e-94675dc0ecc0" class="code code-wrap"><code>class MultitaskTransformer(nn.Module):
    def __init__(self, 
                 num_items=381,
                 hidden_size=128,
                 num_layers=3, 
                 mlp_size=64, # normally = 4 * hidden_size
                 qkv_size=32, # normally = 64 = hidden_size / num_heads
                 num_heads=4, 
                 msa_dropout_ratio=0.1, 
                 ffn_dropout_ratio=0.1, 
                 device=&#x27;cpu&#x27;
                ):
        super().__init__()
        self.device = device
        self.num_items = num_items
        self.NUM_ITEM_DISCRETE_FEATURE = 3+1 + 2+1 # item_vec3+location1 + item_vec2+price1
        self.NUM_USER_DISCRETE_FEATURE = 10
        self.hidden_size = hidden_size
        self.N_buy = 1 + self.NUM_ITEM_DISCRETE_FEATURE + \
                     9 * (1 + self.NUM_ITEM_DISCRETE_FEATURE) + \
                     self.NUM_USER_DISCRETE_FEATURE
        self.N_click = 1 + self.NUM_ITEM_DISCRETE_FEATURE + \
                       1 + self.NUM_ITEM_DISCRETE_FEATURE + \
                       self.NUM_USER_DISCRETE_FEATURE

        # item emb
        self.item_emb = nn.Embedding(self.num_items + 1, self.hidden_size) # num_items + 1

        # item discrete feature
        self.item_discrete_feature_emb_list = nn.ModuleList()
        num_unique_value_list = [4+1, 10+1, 2+1, 3+1, 5+1, 10+1, 11+1] # [4, 10, 2, 3]
        for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
            num_unique_value = num_unique_value_list[i]
            self.item_discrete_feature_emb_list.append(
                nn.Embedding(num_unique_value, self.hidden_size)
            )
        
        # user discrete feature
        self.user_discrete_feature_emb_list = nn.ModuleList()
        num_unique_value_list = [4, 1364, 21, 11, 196, 50, 4, 12, 3, 2165] # (already add 1 for features in test but not in train)
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            num_unique_value = num_unique_value_list[i]
            self.user_discrete_feature_emb_list.append(
                nn.Embedding(num_unique_value, self.hidden_size)
            )
        
        # position emb
        self.position_emb_buy   = nn.Parameter(torch.randn(1, self.N_buy, self.hidden_size))
        self.position_emb_click = nn.Parameter(torch.randn(1, self.N_click, self.hidden_size))

        # transformer layers
        self.transformer_layers_buy = nn.ModuleList([])
        for _ in range(num_layers):
            self.transformer_layers_buy.append(nn.ModuleList([
                nn.Sequential( # MSA(LN(x))
                    nn.LayerNorm(self.hidden_size),
                    MultiHeadSelfAttention(self.hidden_size, qkv_size, num_heads, msa_dropout_ratio),
                ),
                nn.Sequential( # MLPs(LN(x))
                    nn.LayerNorm(self.hidden_size),
                    FeedForwardNetwork(self.hidden_size, mlp_size, ffn_dropout_ratio)
                )
            ]))
        self.transformer_layers_click = nn.ModuleList([])
        for _ in range(num_layers):
            self.transformer_layers_click.append(nn.ModuleList([
                nn.Sequential( # MSA(LN(x))
                    nn.LayerNorm(self.hidden_size),
                    MultiHeadSelfAttention(self.hidden_size, qkv_size, num_heads, msa_dropout_ratio),
                ),
                nn.Sequential( # MLPs(LN(x))
                    nn.LayerNorm(self.hidden_size),
                    FeedForwardNetwork(self.hidden_size, mlp_size, ffn_dropout_ratio)
                )
            ]))

        # session prediction head
        self.session_prediction_head = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.PReLU(),
            nn.Linear(64, 4)
        )

        # buy prediction head
        self.buy_prediction_head = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.PReLU(),
            nn.Linear(64, 9)
        )

        # click prediction head
        self.click_prediction_head = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.PReLU(),
            nn.Linear(64, 1)
        )


    def get_item_emb_attr(self, 
                          item_id, 
                          item_discrete_feature):
        &quot;&quot;&quot;
        param:
            item_id:               [B, 9]
            item_discrete_feature: [B, 9, NUM_USER_DISCRETE_FEATURE]
        return: 
            emb_attr:
                [B(batchsize), 9, N(num_feat=1+7), D(hiddendim)]
        note: 
            above, 9 can be an arbitrary number, e.g. 400
        &quot;&quot;&quot;
        tmp = []
        # item emb
        item_emb = self.item_emb(item_id) # [B, 9, D]
        tmp.append(torch.unsqueeze(item_emb, 2)) # [B, 9, 1, D]
        # item discrete feature emb
        for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
            a = self.item_discrete_feature_emb_list[i](item_discrete_feature[:, :, i]) # [B, 9, D]
            tmp.append(torch.unsqueeze(a, 2)) # [B, 9, 1, D]
        # cat to [B, 9, N, D]
        return torch.cat(tmp, dim=2) # [B, 9, 8, D]

    def forward(self,
                user_click_history, user_click_history_discrete_feature, num_user_click_history,
                nine_item_id, nine_item_discrete_feature,
                user_discrete_feature,
                ):
        &quot;&quot;&quot;
            user_click_history: [N, 400]
            user_click_history_discrete_feature: [N, 400, 3+1 + 2+1]
            num_user_click_history: [N, 1]
            nine_item_id: [N, 9]
            nine_item_discrete_feature: [N, 9, 3+1 + 2+1] item_vec3 + location1 + item_vec2 + price1
            user_discrete_feature: [B, 10]
        &quot;&quot;&quot;

        batch_size = user_click_history.size()[0]

        user_click_history_emb = torch.zeros( # [B, 8, D]
            (batch_size, 1 + self.NUM_ITEM_DISCRETE_FEATURE, self.hidden_size)
        ).to(self.device)
        assert 1 + self.NUM_ITEM_DISCRETE_FEATURE == 8
        tmp = self.get_item_emb_attr(user_click_history, user_click_history_discrete_feature) # [B, 400, 8, D]
        for i in range(batch_size):
            aa = tmp[i, :num_user_click_history[i], :, :] # [B, 400, 8, D] -&gt; [400-, 8, D]
            a = torch.mean(aa, dim=0) # [400-, 8, D] -&gt; [8, D]
            user_click_history_emb[i] = a
        
        nine_item_emb = self.get_item_emb_attr(nine_item_id, nine_item_discrete_feature) # [B, 9, 8, D]
        nine_item_emb = einops.rearrange(nine_item_emb, &#x27;B n N D -&gt; B (n N) D&#x27;) # [B, 9*8, D]

        tmp = []
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            a = self.user_discrete_feature_emb_list[i](user_discrete_feature[:, i]) # [B, D]
            tmp.append(torch.unsqueeze(a, 1)) # [B, 1, D]
        user_discrete_feature_emb = torch.cat(tmp, dim=1) # [B, 10, D]

        # concat all emb
        z0 = torch.cat([user_click_history_emb,     # [B, 8, D]
                        nine_item_emb,              # [B, 9*8, D]
                        user_discrete_feature_emb,  # [B, 10, D]
                        ], dim=1) # [B, N, D]

        position_embs = einops.repeat(self.position_emb_buy, &#x27;() N D -&gt; B N D&#x27;, B=batch_size)
        z0 = z0 + position_embs

        # transformer
        zl = z0
        for transformer_layer in self.transformer_layers_buy:
            zl = zl + transformer_layer[0](zl) # MSA(LN(x))
            zl = zl + transformer_layer[1](zl) # MLPs(LN(x))

        # global average pooling
        zl = einops.reduce(zl, &#x27;B N D -&gt; B D&#x27;, reduction=&#x27;mean&#x27;)

        # head
        session_pred = self.session_prediction_head(zl)
        buy_pred = self.buy_prediction_head(zl)

        return session_pred, buy_pred # [B, 4], [B, 9]


    def forward_click(self,
                      user_click_history, user_click_history_discrete_feature, num_user_click_history,
                      item_id, item_discrete_feature,
                      user_discrete_feature):
        batch_size = user_click_history.size()[0]

        user_click_history_emb = torch.zeros( # [B, 7+1, D]
            (batch_size, 1 + self.NUM_ITEM_DISCRETE_FEATURE, self.hidden_size)
        ).to(self.device)
        assert 1 + self.NUM_ITEM_DISCRETE_FEATURE == 8
        # print(user_click_history.device, user_click_history_discrete_feature.device, flush=True)
        tmp = self.get_item_emb_attr(user_click_history, user_click_history_discrete_feature) # [B, 400, 8, D]
        for i in range(batch_size):
            aa = tmp[i, :num_user_click_history[i], :, :] # [B, 400, 8, D] -&gt; [400-, 8, D]
            a = torch.mean(aa, dim=0) # [400-, 8, D] -&gt; [8, D]
            user_click_history_emb[i] = a
        
        item_discrete_feature = torch.unsqueeze(item_discrete_feature, dim=1) # [B, 7] -&gt; [B, 1, 7]
        # print(item_id.shape, item_discrete_feature.shape)
        item_emb = self.get_item_emb_attr(item_id, item_discrete_feature) # [B, 1, 8, D]
        item_emb = einops.rearrange(item_emb, &#x27;B n N D -&gt; B (n N) D&#x27;) # [B, 1*8, D]

        tmp = []
        for i in range(self.NUM_USER_DISCRETE_FEATURE):
            a = self.user_discrete_feature_emb_list[i](user_discrete_feature[:, i]) # [B, D]
            tmp.append(torch.unsqueeze(a, 1)) # [B, 1, D]
        user_discrete_feature_emb = torch.cat(tmp, dim=1) # [B, 10, D]

        # concat all emb
        z0 = torch.cat([user_click_history_emb,     # [B, 8, D]
                        item_emb,              # [B, 1*8, D]
                        user_discrete_feature_emb,  # [B, 10, D]
                        ], dim=1) # [B, N, D]

        position_embs = einops.repeat(self.position_emb_click, &#x27;() N D -&gt; B N D&#x27;, B=batch_size)
        z0 = z0 + position_embs

        # transformer
        zl = z0
        for transformer_layer in self.transformer_layers_click:
            zl = zl + transformer_layer[0](zl) # MSA(LN(x))
            zl = zl + transformer_layer[1](zl) # MLPs(LN(x))

        # global average pooling
        zl = einops.reduce(zl, &#x27;B N D -&gt; B D&#x27;, reduction=&#x27;mean&#x27;)

        # head
        click_pred = self.click_prediction_head(zl)

        return click_pred # [B, 1]</code></pre><p id="05786910-c7d4-433a-81b7-70c8262300b8" class="">A dry run of this model would give output like this:</p><figure id="8f13fb35-4865-48bc-b658-a95f890ba3db" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2035.png"><img style="width:826px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2035.png"/></a></figure><h2 id="3f79e649-1b92-4b36-a743-97d2da9edd75" class="">Multi-Task Transformer with User Buy Time Augmentation</h2><p id="209970ae-7919-42a4-af42-3c23e5a9464a" class="">We will implement multitask model - buy and click.</p><p id="a9c4daa2-0a62-40f6-b5c2-a74bf523fa89" class="">Model 1:</p><ul id="fc4af4b1-bbef-4021-8639-ddf58a11ec1c" class="bulleted-list"><li style="list-style-type:disc">Input<ul id="c3e4903f-3d9d-449b-864c-b5506b66b5f7" class="bulleted-list"><li style="list-style-type:circle">9 products displayed to users (emb, discrete attributes, continuous attributes of products)</li></ul><ul id="4f6cfb97-5ee7-40d0-ba43-43bbd9cd9296" class="bulleted-list"><li style="list-style-type:circle">User&#x27;s click history (emb, discrete attributes, continuous attributes of products)</li></ul><ul id="1022be34-d022-43cb-8ef8-0a94a11a2baa" class="bulleted-list"><li style="list-style-type:circle">User attributes (discrete attributes)</li></ul><ul id="ec25eba7-4cda-4e30-be34-84530896a9e7" class="bulleted-list"><li style="list-style-type:circle">User purchase time (month, day, day of the week, hour)</li></ul></li></ul><ul id="6f3e999c-2caf-4395-ab9d-3ee6bc2d6c8a" class="bulleted-list"><li style="list-style-type:disc">Output<ul id="8755c9b0-ae67-44f9-ae99-55fd13502f17" class="bulleted-list"><li style="list-style-type:circle">The user purchased the first session (purchased 0-3, 4-6, 7-9, three types of sessions)</li></ul><ul id="292697da-80dd-4200-aad9-2a6d56a7e623" class="bulleted-list"><li style="list-style-type:circle">Whether the user bought these 9 products (you can use the 4 types of product reweighting loss mentioned by Gaochen)</li></ul></li></ul><p id="55ac865f-f698-4be2-9060-245693127f99" class="">Model 2:</p><ul id="184e500b-a399-4d1e-982a-60b489a4f555" class="bulleted-list"><li style="list-style-type:disc">Input<ul id="bf299aed-a862-4324-939d-ebcf4d3a476f" class="bulleted-list"><li style="list-style-type:circle">User’s previous click history (commodity emb, discrete attributes, and continuous attributes become discrete)</li></ul><ul id="4a2f83e3-b0d4-4fcd-a9b7-3f385979ea78" class="bulleted-list"><li style="list-style-type:circle">The product currently clicked by the user (the emb, discrete attributes, and continuous attributes of the product become discrete)</li></ul><ul id="73d3d2b5-57b1-4598-97e4-eeeeaa75d8b7" class="bulleted-list"><li style="list-style-type:circle">User attributes (discrete attributes)</li></ul></li></ul><ul id="d379fe82-7126-480e-8e16-3c133e99275a" class="bulleted-list"><li style="list-style-type:disc">Output<ul id="25947d30-08ac-4a10-a3a2-f8b5bdd0f6ce" class="bulleted-list"><li style="list-style-type:circle">Whether the user clicked on this product</li></ul></li></ul><p id="c5aaddf1-a1f6-4f53-8b86-f4df879f9efc" class="">The above two models share all emb.</p><h3 id="abb29175-bd82-4028-9c1a-e75e74759296" class="">Implementation</h3><pre id="52ff5ada-145c-46b2-8edf-f0489c14bf99" class="code code-wrap"><code># item emb
self.item_emb = nn.Embedding(self.num_items + 1, self.hidden_size)

# item discrete feature
self.item_discrete_feature_emb_list = nn.ModuleList()
num_unique_value_list = [4+1, 10+1, 2+1, 3+1, 5+1, 10+1, 11+1] # [4, 10, 2, 3]
for i in range(self.NUM_ITEM_DISCRETE_FEATURE):
    num_unique_value = num_unique_value_list[i]
    self.item_discrete_feature_emb_list.append(
        nn.Embedding(num_unique_value, self.hidden_size)
    )

# user discrete feature
self.user_discrete_feature_emb_list = nn.ModuleList()
num_unique_value_list = [4, 1364, 21, 11, 196, 50, 4, 12, 3, 2165] # (already add 1 for features in test but not in train)
for i in range(self.NUM_USER_DISCRETE_FEATURE):
    num_unique_value = num_unique_value_list[i]
    self.user_discrete_feature_emb_list.append(
        nn.Embedding(num_unique_value, self.hidden_size)
    )

# position emb
self.position_emb_buy   = nn.Parameter(torch.randn(1, self.N_buy, self.hidden_size))
self.position_emb_click = nn.Parameter(torch.randn(1, self.N_click, self.hidden_size))

# time emb
self.month_emb   = nn.Embedding(12, self.hidden_size)
self.date_emb    = nn.Embedding(31, self.hidden_size)
self.weekday_emb = nn.Embedding(7,  self.hidden_size)
self.hour_emb    = nn.Embedding(24, self.hidden_size)

# transformer layers
self.transformer_layers_buy = nn.ModuleList([])
for _ in range(num_layers):
    self.transformer_layers_buy.append(nn.ModuleList([
        nn.Sequential( # MSA(LN(x))
            nn.LayerNorm(self.hidden_size),
            MultiHeadSelfAttention(self.hidden_size, qkv_size, num_heads, msa_dropout_ratio),
        ),
        nn.Sequential( # MLPs(LN(x))
            nn.LayerNorm(self.hidden_size),
            FeedForwardNetwork(self.hidden_size, mlp_size, ffn_dropout_ratio)
        )
    ]))
self.transformer_layers_click = nn.ModuleList([])
for _ in range(num_layers):
    self.transformer_layers_click.append(nn.ModuleList([
        nn.Sequential( # MSA(LN(x))
            nn.LayerNorm(self.hidden_size),
            MultiHeadSelfAttention(self.hidden_size, qkv_size, num_heads, msa_dropout_ratio),
        ),
        nn.Sequential( # MLPs(LN(x))
            nn.LayerNorm(self.hidden_size),
            FeedForwardNetwork(self.hidden_size, mlp_size, ffn_dropout_ratio)
        )
    ]))

# session prediction head
self.session_prediction_head = nn.Sequential(
    nn.Linear(self.hidden_size, 64),
    nn.PReLU(),
    nn.Linear(64, 4)
)

# buy prediction head
self.buy_prediction_head = nn.Sequential(
    nn.Linear(self.hidden_size, 64),
    nn.PReLU(),
    nn.Linear(64, 9)
)

# click prediction head
self.click_prediction_head = nn.Sequential(
    nn.Linear(self.hidden_size, 64),
    nn.PReLU(),
    nn.Linear(64, 1)
)</code></pre><p id="b0f5b359-c36c-4d4c-8969-4e83e5c9ea66" class="">A dry run of this model would give output like this:</p><figure id="0e0c0307-63f6-468a-ae13-aa4a8e869437" class="image"><a href="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2036.png"><img style="width:831px" src="Providing%20Bundled%20Product%20Recommendations%20to%20Onlin%208febff62eb714ebd848a8f319645ba9d/Untitled%2036.png"/></a></figure><h2 id="89fa351d-da79-43fa-bba1-c4e09f5079c8" class="">Augmented Multi-Task Transformer Model</h2><p id="d423d5b4-108e-4b8f-a71e-f5613878b6e4" class="">We propose a delicate two-headed transformer-based framework to predict both users’ buying behavior and unlocked sessions. The unlocked session prediction can be used to refine unreasonable buy predictions. We further propose a randomness-in-session augmentation technique and a novel session-aware reweighted loss to address the unique characteristics in this scenario. Finally, a multi-tasking training procedure with click prediction is utilized to assist the learning of embedding layers.</p><p id="9acdaf22-f1a0-4906-ad22-ad75f0b53015" class="">Model 1:</p><ul id="5169ac2b-6366-4a2b-a2ec-2ed819f92d6b" class="bulleted-list"><li style="list-style-type:disc">Input<ul id="163c85c0-5e7f-456f-a1b2-daf1e9f82bcf" class="bulleted-list"><li style="list-style-type:circle">9 products displayed to users (the emb, discrete attributes, and continuous attributes of the products become discrete)</li></ul><ul id="0eff5ed6-1cf2-4c2b-9f81-3d15f572ab24" class="bulleted-list"><li style="list-style-type:circle">User’s click history (commodity emb, discrete attributes, and continuous - attributes become discrete)</li></ul><ul id="f801bee3-1901-4a3f-9d7f-de39b30b6e6f" class="bulleted-list"><li style="list-style-type:circle">User attributes (discrete attributes)</li></ul><ul id="8c92fa59-cfd5-4acd-b457-b6e60cab5bf5" class="bulleted-list"><li style="list-style-type:circle">User id (emb)</li></ul><ul id="b6e1ba85-33c3-49e2-8969-7f3f9447a4a1" class="bulleted-list"><li style="list-style-type:circle">User purchase time (month, day, day of the week, hour)</li></ul></li></ul><ul id="5d71c705-0ef1-4582-a44d-a02ff5d8ce85" class="bulleted-list"><li style="list-style-type:disc">Output<ul id="d19682fe-6852-484d-be5e-2c422f3a4fc5" class="bulleted-list"><li style="list-style-type:circle">The user purchased the first session (purchased 0-3, 4-6, 7-9, three types of sessions)</li></ul><ul id="b4281cce-02ed-4b59-a37a-adf156a69763" class="bulleted-list"><li style="list-style-type:circle">Whether the user bought these 9 products (you can use the 4 types of product reweighting loss mentioned by Gaochen)</li></ul></li></ul><p id="a051c8d6-c266-4faa-b7a5-afebf288dbbc" class="">Model 2:</p><ul id="0692fa47-f171-4fd9-8d57-d3a2755e0df2" class="bulleted-list"><li style="list-style-type:disc">Input<ul id="610d3938-f1c6-444e-bb2c-3ce911c2db63" class="bulleted-list"><li style="list-style-type:circle">User before clicking History (commodity emb, discrete properties, property becomes continuous discrete)</li></ul><ul id="23db2737-b25e-4895-ac07-05eb482210dd" class="bulleted-list"><li style="list-style-type:circle">Users current click commodities (emb goods, discrete properties, property becomes continuous discrete)</li></ul><ul id="5e917fab-1b5a-41cf-8fe5-8721fad634a5" class="bulleted-list"><li style="list-style-type:circle">User attributes (discrete attributes)</li></ul><ul id="911bd5b8-b567-4579-bdf9-2ebc9682762c" class="bulleted-list"><li style="list-style-type:circle">User id (emb)</li></ul></li></ul><ul id="2cecb4ff-ec64-4204-8d79-1fb8185d04f5" class="bulleted-list"><li style="list-style-type:disc">Output<ul id="761d53ed-7609-4971-a388-ec9abb03c190" class="bulleted-list"><li style="list-style-type:circle">Whether the user clicked on this product</li></ul></li></ul><p id="c576c28d-c4de-489d-8563-114f47159dab" class="">The above two models share all emb.</p><h2 id="6b93079a-3ec6-4394-82a4-333b3021aa40" class="">Model Summary</h2><table id="a1a84362-c021-402f-b2df-ef94581eb2ae" class="simple-table"><thead><tr id="bb6a467d-238a-41e7-9f1a-98fda553ba8e"><th id="rFbz" class="simple-table-header">Model</th><th id="aXR:" class="simple-table-header">Module</th><th id="{Wwg" class="simple-table-header">Validation</th><th id="psAS" class="simple-table-header">Test</th></tr></thead><tbody><tr id="1d45dccc-e31d-4ccc-824c-9ea4798b5c17"><td id="rFbz">A</td><td id="aXR:">MLP basic model</td><td id="{Wwg">0.29169</td><td id="psAS">0.33817</td></tr><tr id="49d57255-45c7-4858-b36d-c224aae4a156"><td id="rFbz">B</td><td id="aXR:">+ randomness-in-session augmentation (train)</td><td id="{Wwg">0.29965</td><td id="psAS">0.35007</td></tr><tr id="1f6b1c25-7cac-4dc2-bdc0-72b0876d40f5"><td id="rFbz">C</td><td id="aXR:">+ transformer backbone</td><td id="{Wwg">0.31140</td><td id="psAS">0.36210</td></tr><tr id="e31ef330-55ac-41eb-a396-059f77641848"><td id="rFbz">D</td><td id="aXR:">+ two-headed (buy and group) prediction</td><td id="{Wwg">0.31475</td><td id="psAS">0.36258</td></tr><tr id="d0078a15-6bd3-4149-bdcf-e8397b7466b8"><td id="rFbz">E</td><td id="aXR:">+ session-aware loss reweighting</td><td id="{Wwg">0.33090</td><td id="psAS">0.38355</td></tr><tr id="8851e863-cd38-4c4d-9046-f559897c657f"><td id="rFbz">F</td><td id="aXR:">+ multi-tasking with click prediction</td><td id="{Wwg">0.33323</td><td id="psAS">0.38805</td></tr></tbody></table><h2 id="3a14e224-c248-4490-8d8b-1f4651458062" class="">Notebooks</h2><table id="c593a0c4-f3ad-496e-be99-6b60185ac2bf" class="simple-table"><thead><tr id="9203d7da-ea2f-4671-9597-a8e73425812c"><th id="mL&gt;t" class="simple-table-header">Title</th><th id="RNi^" class="simple-table-header">Notebook Link</th></tr></thead><tbody><tr id="3632d055-1646-4ca9-a317-c8dd858cba86"><td id="mL&gt;t">Data Exploration</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/b97030fee2638d8faa1bced0a9059cec">https://nbviewer.org/gist/sparsh-ai/b97030fee2638d8faa1bced0a9059cec</a></td></tr><tr id="7a64fe20-524c-438c-9d3a-376f99df849b"><td id="mL&gt;t">Data Preparation</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/8a97a5a7ef43fb114082f5a0af4f5f30">https://nbviewer.org/gist/sparsh-ai/8a97a5a7ef43fb114082f5a0af4f5f30</a></td></tr><tr id="3fc85f0d-6836-46ba-84bb-c3242bced3dd"><td id="mL&gt;t">MLP Model</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/d5157506a572788da2846c2558b7a184">https://nbviewer.org/gist/sparsh-ai/d5157506a572788da2846c2558b7a184</a></td></tr><tr id="bd0b60fc-09de-44ff-a39a-fa2833eb27f4"><td id="mL&gt;t">MLP + Session Predictions</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/2706e3b6350753c9c06ad5a927b2e92b">https://nbviewer.org/gist/sparsh-ai/2706e3b6350753c9c06ad5a927b2e92b</a></td></tr><tr id="a6bc2d5d-5029-4c6c-8382-ee22d9e39949"><td id="mL&gt;t">Feature Layer Augmentation</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/09b7425f837cc188d9bbc73c85dc1dd4">https://nbviewer.org/gist/sparsh-ai/09b7425f837cc188d9bbc73c85dc1dd4</a></td></tr><tr id="af4ae7ca-6901-4836-b3d0-07098cc604a9"><td id="mL&gt;t">Transformer Backbone</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/7fdf8ad4f170d630b8e9bf28263d9916">https://nbviewer.org/gist/sparsh-ai/7fdf8ad4f170d630b8e9bf28263d9916</a></td></tr><tr id="2692a117-5238-4089-ae1a-3f6c4b6f727b"><td id="mL&gt;t">Multi-Task Transformer</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/f0e3a9518148af07b6828d84b90e93e1">https://nbviewer.org/gist/sparsh-ai/f0e3a9518148af07b6828d84b90e93e1</a></td></tr><tr id="09d08e61-66fc-436f-a66e-2d1e8c8b2b4a"><td id="mL&gt;t">User Buy-time Augmentation</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/566fe8319abcb63b73a6cb1784a5d806">https://nbviewer.org/gist/sparsh-ai/566fe8319abcb63b73a6cb1784a5d806</a></td></tr><tr id="bdff6b41-38fa-4e9b-a2f7-f010d1e43927"><td id="mL&gt;t">Augmented Transformer</td><td id="RNi^"><a href="https://nbviewer.org/gist/sparsh-ai/bc86b9ac16dbd3f10aa0b60b3d6ff3be">https://nbviewer.org/gist/sparsh-ai/bc86b9ac16dbd3f10aa0b60b3d6ff3be</a></td></tr></tbody></table><p id="9ba604c7-08c1-4275-a207-802d7132ac1e" class="">
</p></div></article></body></html>